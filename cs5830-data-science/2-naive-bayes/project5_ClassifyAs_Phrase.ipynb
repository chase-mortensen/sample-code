{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the code for using 2 or three consicutive words (as phrase) for prediction. For the main implemintation using only words, please see the other notebook named project5_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from statistics import mode \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_name</th>\n",
       "      <th>debate_section</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speech</th>\n",
       "      <th>speaking_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Part 1</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Candidates, welcome. Vice President Biden, the...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Part 1</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Oh, they didn’t miss anything. It’s a long rac...</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Part 1</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Why are Senator Sanders and Mayor Buttigieg to...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Part 1</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Well, you know that with regard to Senator San...</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Part 1</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Senator Sanders, let me give you the chance to...</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  debate_name debate_section    speaker  \\\n",
       "0  New Hampshire Democratic Debate Transcript         Part 1  George S.   \n",
       "1  New Hampshire Democratic Debate Transcript         Part 1  Joe Biden   \n",
       "2  New Hampshire Democratic Debate Transcript         Part 1  George S.   \n",
       "3  New Hampshire Democratic Debate Transcript         Part 1  Joe Biden   \n",
       "4  New Hampshire Democratic Debate Transcript         Part 1  George S.   \n",
       "\n",
       "                                              speech  speaking_time_seconds  \n",
       "0  Candidates, welcome. Vice President Biden, the...                   18.0  \n",
       "1  Oh, they didn’t miss anything. It’s a long rac...                   36.0  \n",
       "2  Why are Senator Sanders and Mayor Buttigieg to...                    4.0  \n",
       "3  Well, you know that with regard to Senator San...                   41.0  \n",
       "4  Senator Sanders, let me give you the chance to...                   21.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('debate_transcripts.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['George S.', 'Joe Biden', 'Bernie Sanders', 'Amy Klobuchar',\n",
       "       'Tom Steyer', 'Andrew Yang', 'Elizabeth Warren', 'Pete Buttigieg',\n",
       "       'Speaker 1', 'Linsey Davis', 'David Muir', 'Monica Hernandez',\n",
       "       'Adam Sexton', 'Devin Dwyer', 'Rachel Scott', 'Announcer',\n",
       "       'Wolf Blitzer', 'Abby Phillips', 'B. Pfannenstiel', 'Moderator 1',\n",
       "       'Moderator 2', 'Brianne P.', 'Judy Woodruff', 'Amy Walter',\n",
       "       'Stephanie Sy', 'Speaker 2', 'Tim Alberta', 'Amna Nawaz',\n",
       "       'Yamiche A.', 'Rachel Maddow', 'Andrea Mitchell', 'Kamala Harris',\n",
       "       'Cory Booker', 'Kristen Welker', 'Ashley Parker', 'Tulsi Gabbard',\n",
       "       'Speaker 3', 'Anderson Cooper', 'Erin Burnett', 'Marc Lacey',\n",
       "       'Julian Castro', 'Beto O’Rourke', 'A. Cooper', 'Jake Tapper',\n",
       "       'Voiceover', 'Jorge Ramos', 'Sec. Castro', 'Speaker 13',\n",
       "       'Dana Bash', 'Bill de Blasio', 'Michael Bennet', 'Jay Inslee',\n",
       "       'Kirsten Gillibrand', 'Don Lemon', 'Crowd', 'Kirseten Gillibrand',\n",
       "       'Moderator', 'Speaker 12', 'Diana', 'Steve Bullock',\n",
       "       'Marianne Williamson', 'John Delaney', 'Tim Ryan', 'John H.',\n",
       "       'Female', 'Male', 'John Hickenloop', 'Speaker 9',\n",
       "       'J. Hickenlooper', 'John King', 'N. Henderson', 'Savanagh G.',\n",
       "       'Bennett', 'Jose D.B.', 'Eric Stalwell', 'Eric Swalwell',\n",
       "       'Speaker 4', 'Speaker 5', 'Speaker 6', 'Speaker 7', 'Lester Holt',\n",
       "       'Speaker 8', 'Savannah G.', 'Chuck Todd', 'Steve Kornacki',\n",
       "       'Speaker 16', 'Speaker 17', 'Speaker 15', 'Speaker 18',\n",
       "       'Speaker 19', 'Speaker 20', 'Speaker 10', 'Speaker 11',\n",
       "       'Speaker 14'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.speaker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michael bloomberg not included because Nevada debate not included in data set\n",
    "# Other current candidates have been removed due to not being in the latest debates (like Tulsi Gabbard)\n",
    "# Candidates that have dropped out are not included\n",
    "\n",
    "top_candidates = ['Joe Biden', 'Bernie Sanders', 'Amy Klobuchar', 'Tom Steyer', 'Elizabeth Warren', \n",
    "                  'Pete Buttigieg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speech</th>\n",
       "      <th>speaking_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Candidates, welcome. Vice President Biden, the...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Oh, they didn’t miss anything. It’s a long rac...</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Why are Senator Sanders and Mayor Buttigieg to...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Well, you know that with regard to Senator San...</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Senator Sanders, let me give you the chance to...</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  debate_name    speaker  \\\n",
       "0  New Hampshire Democratic Debate Transcript  George S.   \n",
       "1  New Hampshire Democratic Debate Transcript  Joe Biden   \n",
       "2  New Hampshire Democratic Debate Transcript  George S.   \n",
       "3  New Hampshire Democratic Debate Transcript  Joe Biden   \n",
       "4  New Hampshire Democratic Debate Transcript  George S.   \n",
       "\n",
       "                                              speech  speaking_time_seconds  \n",
       "0  Candidates, welcome. Vice President Biden, the...                   18.0  \n",
       "1  Oh, they didn’t miss anything. It’s a long rac...                   36.0  \n",
       "2  Why are Senator Sanders and Mayor Buttigieg to...                    4.0  \n",
       "3  Well, you know that with regard to Senator San...                   41.0  \n",
       "4  Senator Sanders, let me give you the chance to...                   21.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=\"debate_section\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speech</th>\n",
       "      <th>speaking_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Oh, they didn’t miss anything. It’s a long rac...</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Well, you know that with regard to Senator San...</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>Because Donald Trump lies all the time. It doe...</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>I believe that the way we beat Trump is by hav...</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>That’s true. And that’s the disappointment and...</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  debate_name         speaker  \\\n",
       "1  New Hampshire Democratic Debate Transcript       Joe Biden   \n",
       "3  New Hampshire Democratic Debate Transcript       Joe Biden   \n",
       "5  New Hampshire Democratic Debate Transcript  Bernie Sanders   \n",
       "6  New Hampshire Democratic Debate Transcript  Bernie Sanders   \n",
       "8  New Hampshire Democratic Debate Transcript  Bernie Sanders   \n",
       "\n",
       "                                              speech  speaking_time_seconds  \n",
       "1  Oh, they didn’t miss anything. It’s a long rac...                   36.0  \n",
       "3  Well, you know that with regard to Senator San...                   41.0  \n",
       "5  Because Donald Trump lies all the time. It doe...                   41.0  \n",
       "6  I believe that the way we beat Trump is by hav...                   39.0  \n",
       "8  That’s true. And that’s the disappointment and...                   23.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.loc[data['speaker'].isin(top_candidates)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Taking off Test Data (10% in random, training in other 10%)\n",
    "import math\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()\n",
    "numData =len(data)\n",
    "testData = data[:(math.floor(numData*0.1))]\n",
    "data = data[(math.floor(numData*0.1)+1):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most frequent words for each candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Joe Biden': 21184,\n",
       " 'Bernie Sanders': 16636,\n",
       " 'Amy Klobuchar': 17036,\n",
       " 'Tom Steyer': 8547,\n",
       " 'Elizabeth Warren': 20373,\n",
       " 'Pete Buttigieg': 18173}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqWordsPair = {}\n",
    "wordCountsPair = {}\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    candidate_data = data.loc[data['speaker'] == candidate]\n",
    "    freqWordsPair[candidate] = {}\n",
    "    words = 0\n",
    "    for line in candidate_data['speech']:\n",
    "        line = line.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\")\n",
    "        wordList = line.split()\n",
    "        line = zip(wordList, wordList[1:])\n",
    "        for word in line:\n",
    "            words += 1\n",
    "#             word = word.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\")\n",
    "            if word in (freqWordsPair[candidate]):\n",
    "                freqWordsPair[candidate][word] += 1\n",
    "            else:\n",
    "                freqWordsPair[candidate][word] = 1\n",
    "    wordCountsPair[candidate] = words\n",
    "                \n",
    "    freqWordsPair[candidate] = sorted(freqWordsPair[candidate].items(), key = lambda kv:(kv[1], kv[0]))\n",
    "    freqWordsPair[candidate].reverse()\n",
    "                \n",
    "# for candidate in freqWords:\n",
    "#     print(candidate, freqWords[candidate][:50])\n",
    "wordCountsPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(freqWordsPair['Elizabeth Warren'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Joe Biden  -----\n",
      "[(('of', 'the'), 124), (('to', 'be'), 93), (('in', 'the'), 92), (('going', 'to'), 92), (('have', 'to'), 91)]\n",
      "-----  Bernie Sanders  -----\n",
      "[(('of', 'the'), 112), (('we', 'have'), 97), (('this', 'country'), 67), (('in', 'the'), 58), (('on', 'the'), 56)]\n",
      "-----  Amy Klobuchar  -----\n",
      "[(('and', 'i'), 101), (('i', 'think'), 89), (('in', 'the'), 74), (('of', 'the'), 70), (('going', 'to'), 61)]\n",
      "-----  Tom Steyer  -----\n",
      "[(('going', 'to'), 75), (('have', 'to'), 62), (('to', 'have'), 41), (('of', 'the'), 37), (('we’re', 'going'), 35)]\n",
      "-----  Elizabeth Warren  -----\n",
      "[(('we', 'need'), 101), (('going', 'to'), 96), (('need', 'to'), 89), (('we', 'have'), 77), (('of', 'the'), 71)]\n",
      "-----  Pete Buttigieg  -----\n",
      "[(('of', 'the'), 107), (('in', 'the'), 76), (('to', 'be'), 67), (('going', 'to'), 62), (('this', 'is'), 51)]\n"
     ]
    }
   ],
   "source": [
    "for candidate in freqWordsPair:\n",
    "    print(\"----- \", candidate, \" -----\")\n",
    "#     topWordsVar = candidate.substr(0, text.indexOf(' '))\n",
    "#     topWordsVar = topWordsVar +'TopWords'\n",
    "#     print(topWordsVar)\n",
    "    print(freqWordsPair[candidate][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "# for candidates in \n",
    "# reduce(set.intersection, [set(l_) for l_ in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordProportions = {}\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    wordProportions[candidate] = {}\n",
    "    for word in freqWordsPair[candidate]:\n",
    "        wordProportions[candidate][word[0]] = word[1]/wordCountsPair[candidate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordProportions['Bernie Sanders']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: loop through each word in a sentence and add the probability of each word to a candidate's score (using naive bayes) - the candidate with the highest score is guessed as the speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speech</th>\n",
       "      <th>speaking_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Transcript of July Democratic Debate 2nd Round...</td>\n",
       "      <td>Elizabeth Warren</td>\n",
       "      <td>And go up for corporations. For middle class f...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>September Houston Democratic Debate Transcript...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Thank you.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Transcript of July Democratic Debate 2nd Round...</td>\n",
       "      <td>Amy Klobuchar</td>\n",
       "      <td>Yes. This isn’t just about a system, or it’s n...</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Transcript from Night 2 of the First 2019 June...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>A real 30 seconds?</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>December Democratic Debate Transcript: Sixth D...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Bibi Netanyahu and I know one another well. He...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           debate_name           speaker  \\\n",
       "161  Transcript of July Democratic Debate 2nd Round...  Elizabeth Warren   \n",
       "162  September Houston Democratic Debate Transcript...         Joe Biden   \n",
       "163  Transcript of July Democratic Debate 2nd Round...     Amy Klobuchar   \n",
       "164  Transcript from Night 2 of the First 2019 June...         Joe Biden   \n",
       "165  December Democratic Debate Transcript: Sixth D...         Joe Biden   \n",
       "\n",
       "                                                speech  speaking_time_seconds  \n",
       "161  And go up for corporations. For middle class f...                    3.0  \n",
       "162                                         Thank you.                    1.0  \n",
       "163  Yes. This isn’t just about a system, or it’s n...                   43.0  \n",
       "164                                 A real 30 seconds?                    2.0  \n",
       "165  Bibi Netanyahu and I know one another well. He...                   18.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joe Biden           0.216609\n",
       "Elizabeth Warren    0.200000\n",
       "Bernie Sanders      0.189619\n",
       "Pete Buttigieg      0.172318\n",
       "Amy Klobuchar       0.152941\n",
       "Tom Steyer          0.068512\n",
       "Name: speaker, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['speaker'].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerProbabilities = {'Joe Biden':0.217933,\n",
    "                        'Elizabeth Warren':0.200498,\n",
    "                        'Bernie Sanders':0.186800,\n",
    "                        'Pete Buttigieg':0.171233,\n",
    "                        'Amy Klobuchar':0.155044,\n",
    "                        'Tom Steyer':0.068493}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It might be easier to just loop through all words and create a new dataframe with each word and candidate label to work with the built-in naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpList = []\n",
    "tmpList.append([])\n",
    "tmpList.append([])\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    candidate_data = data.loc[data['speaker'] == candidate]\n",
    "    for line in candidate_data['speech']:\n",
    "        line = line.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\")\n",
    "        wordList = line.split()\n",
    "        line = zip(wordList, wordList[1:])\n",
    "        for word in line:\n",
    "            tmpList[0].append(word)\n",
    "            tmpList[1].append(candidate)\n",
    "\n",
    "allWords = pd.DataFrame(np.array(tmpList).T,columns=['word','candidate'])\n",
    "# len(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(thank, you)</td>\n",
       "      <td>Joe Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(a, real)</td>\n",
       "      <td>Joe Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(real, 30)</td>\n",
       "      <td>Joe Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(30, seconds)</td>\n",
       "      <td>Joe Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(bibi, netanyahu)</td>\n",
       "      <td>Joe Biden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  candidate\n",
       "0       (thank, you)  Joe Biden\n",
       "1          (a, real)  Joe Biden\n",
       "2         (real, 30)  Joe Biden\n",
       "3      (30, seconds)  Joe Biden\n",
       "4  (bibi, netanyahu)  Joe Biden"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allWords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101949"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joe Biden           21184\n",
       "Elizabeth Warren    20373\n",
       "Pete Buttigieg      18173\n",
       "Amy Klobuchar       17036\n",
       "Bernie Sanders      16636\n",
       "Tom Steyer           8547\n",
       "Name: candidate, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allWords['candidate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redundent because Train-test data is already splited from dataframe \n",
    "# X_train, X_test, y_train, y_test = train_test_split(allWords['word'], allWords['candidate'], test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word         object\n",
       "candidate    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allWords.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see example code\n",
    "import random\n",
    "# allWords = allWords.sample(frac=1) # words have been shuffled\n",
    "# allWords.head()\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for i in range(len(allWords)):\n",
    "    tmp.append(({'word':allWords['word'][i]},allWords['candidate'][i]))\n",
    "\n",
    "random.shuffle(tmp)\n",
    "# print(type(tmp))\n",
    "\n",
    "training = tmp[:int(len(allWords) * 0.9)]\n",
    "test = tmp[int(len(allWords) * 0.1):]\n",
    "\n",
    "model_0 = nltk.NaiveBayesClassifier.train(training) ### DIFFERENCE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pete Buttigieg\n",
      "Amy Klobuchar\n",
      "Elizabeth Warren\n",
      "Pete Buttigieg\n",
      "Pete Buttigieg\n"
     ]
    }
   ],
   "source": [
    "print(model_0.classify({'word': ('is', 'part')}))\n",
    "print(model_0.classify({'word':('to', 'hear')}))\n",
    "print(model_0.classify({'word':('know', 'we')}))\n",
    "print(model_0.classify({'word':('we', 'saw')}))\n",
    "print(model_0.classify({'word':('part', 'of')}))\n",
    "# print(model_0.classify({'word:':''}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6257097705847093\n"
     ]
    }
   ],
   "source": [
    "# from example code\n",
    "print(nltk.classify.accuracy(model_0, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given 1 word, our accuracy is almost 60-70%\n",
    "\n",
    "## Now, let's see if a vote-type system works better (count up predictions for a speech and see which candidate was most-often predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    candidate_data = testData.loc[testData['speaker'] == candidate]\n",
    "    for line in candidate_data['speech']:\n",
    "        line = line.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\").replace(\"\\\"\", \"\")\n",
    "        wordList = line.split()\n",
    "        line = zip(wordList, wordList[1:], wordList[2:])\n",
    "        words = []\n",
    "        for word in line:\n",
    "            words.append(({'word':word}, candidate))\n",
    "        speeches.append(words)\n",
    "        \n",
    "# tmp[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-Do: Print Collisions\n",
    "\n",
    "from collections import Counter\n",
    "# collision = 0\n",
    "def get_first_mode(a):\n",
    "    c = Counter(a)  \n",
    "    mode_count = max(c.values())\n",
    "    mode = {key for key, count in c.items() if count == mode_count}\n",
    "    first_mode = next(x for x in a if x in mode)\n",
    "    return first_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sp in speeches:\n",
    "    if len(sp)<1:\n",
    "        speeches.remove(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    }
   ],
   "source": [
    "for sp in speeches:\n",
    "    if len(sp)<1:\n",
    "        speeches.remove(sp)\n",
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22580645161290322\n"
     ]
    }
   ],
   "source": [
    "##To-DO: Print number of time with zero hits and defulting to Joe\n",
    "num_def = 0\n",
    "num_correct = 0.0\n",
    "random.shuffle(speeches)\n",
    "for speech in speeches:\n",
    "\n",
    "    word_predictions = []\n",
    "    for word in speech:\n",
    "#         print(word)\n",
    "        word_predictions.append(model_0.classify(word[0]))\n",
    "    \n",
    "    if(len(word_predictions)):\n",
    "        speech_prediction = mode(word_predictions)\n",
    " \n",
    "    if speech_prediction == speech[0][1]:\n",
    "        num_correct +=1\n",
    "\n",
    "print(num_correct / len(speeches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cool, we got it up to ~23%, which I'll consider pretty good considering there are six candidates using a lot of similar language and talking about similar topics\n",
    "\n",
    "Maybe accuracy could be improved by removing common words? - there seems to be a bias in these words to Joe Biden since he had the most speaking time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redundent in word pair\n",
    "\n",
    "# common_words = [\"medicare\",\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n",
    "# num_correct = 0\n",
    "# for speech in speeches:\n",
    "#     word_predictions = []\n",
    "#     for word in speech:\n",
    "#         if word[0]['word'] not in common_words:\n",
    "#             word_predictions.append(model_0.classify(word[0]))\n",
    "\n",
    "#     if (len(word_predictions)):\n",
    "#         speech_prediction = mode(word_predictions)\n",
    "#         if speech_prediction == speech[0][1]:\n",
    "#             num_correct +=1\n",
    "        \n",
    "# print(num_correct / len(speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: predict nevada debate speeches (and possibly south carolina)\n",
    "\n",
    "We could scrape from https://www.rev.com/blog/transcripts/democratic-debate-transcript-las-vegas-nevada-debate\n",
    "and https://www.rev.com/blog/transcripts/south-carolina-democratic-debate-transcript-february-democratic-debate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets look as three word pair for the size of phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speech</th>\n",
       "      <th>speaking_time_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Candidates, welcome. Vice President Biden, the...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Oh, they didn’t miss anything. It’s a long rac...</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Why are Senator Sanders and Mayor Buttigieg to...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>Well, you know that with regard to Senator San...</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Hampshire Democratic Debate Transcript</td>\n",
       "      <td>George S.</td>\n",
       "      <td>Senator Sanders, let me give you the chance to...</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  debate_name    speaker  \\\n",
       "0  New Hampshire Democratic Debate Transcript  George S.   \n",
       "1  New Hampshire Democratic Debate Transcript  Joe Biden   \n",
       "2  New Hampshire Democratic Debate Transcript  George S.   \n",
       "3  New Hampshire Democratic Debate Transcript  Joe Biden   \n",
       "4  New Hampshire Democratic Debate Transcript  George S.   \n",
       "\n",
       "                                              speech  speaking_time_seconds  \n",
       "0  Candidates, welcome. Vice President Biden, the...                   18.0  \n",
       "1  Oh, they didn’t miss anything. It’s a long rac...                   36.0  \n",
       "2  Why are Senator Sanders and Mayor Buttigieg to...                    4.0  \n",
       "3  Well, you know that with regard to Senator San...                   41.0  \n",
       "4  Senator Sanders, let me give you the chance to...                   21.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('debate_transcripts.csv')\n",
    "data = data.drop(columns=\"debate_section\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['speaker'].isin(top_candidates)]\n",
    "data.head()\n",
    "##Taking off Test Data (10% in random, training in other 90%)\n",
    "import math\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()\n",
    "numData =len(data)\n",
    "testData = data[:(math.floor(numData*0.1))]\n",
    "data = data[(math.floor(numData*0.1)+1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqWordsPair = {}\n",
    "wordCountsPair = {}\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    candidate_data = data.loc[data['speaker'] == candidate]\n",
    "    freqWordsPair[candidate] = {}\n",
    "    words = 0\n",
    "    for line in candidate_data['speech']:\n",
    "        line = line.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\")\n",
    "        wordList = line.split()\n",
    "        line = zip(wordList, wordList[1:], wordList[2:])\n",
    "        for word in line:\n",
    "            words += 1\n",
    "#             word = word.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\")\n",
    "            if word in (freqWordsPair[candidate]):\n",
    "                freqWordsPair[candidate][word] += 1\n",
    "            else:\n",
    "                freqWordsPair[candidate][word] = 1\n",
    "    wordCountsPair[candidate] = words\n",
    "                \n",
    "    freqWordsPair[candidate] = sorted(freqWordsPair[candidate].items(), key = lambda kv:(kv[1], kv[0]))\n",
    "    freqWordsPair[candidate].reverse()\n",
    "                \n",
    "# for candidate in freqWords:\n",
    "#     print(candidate, freqWords[candidate][:5])\n",
    "# wordCountsPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Joe Biden  -----\n",
      "[(('we', 'have', 'to'), 57), (('the', 'united', 'states'), 47), (('be', 'able', 'to'), 34), (('to', 'be', 'able'), 25), (('make', 'sure', 'that'), 24)]\n",
      "-----  Bernie Sanders  -----\n",
      "[(('medicare', 'for', 'all'), 33), (('the', 'american', 'people'), 32), (('in', 'this', 'country'), 32), (('have', 'got', 'to'), 30), (('we', 'have', 'got'), 29)]\n",
      "-----  Amy Klobuchar  -----\n",
      "[(('i', 'want', 'to'), 24), (('we', 'have', 'to'), 23), (('and', 'i', 'think'), 23), (('we', 'need', 'to'), 22), (('when', 'it', 'comes'), 19)]\n",
      "-----  Tom Steyer  -----\n",
      "[(('we’re', 'going', 'to'), 34), (('going', 'to', 'have'), 30), (('to', 'have', 'to'), 28), (('on', 'this', 'stage'), 27), (('we', 'have', 'to'), 23)]\n",
      "-----  Elizabeth Warren  -----\n",
      "[(('we', 'need', 'to'), 76), (('we', 'have', 'to'), 33), (('in', 'this', 'country'), 29), (('the', 'united', 'states'), 27), (('i', 'want', 'to'), 27)]\n",
      "-----  Pete Buttigieg  -----\n",
      "[(('the', 'american', 'people'), 22), (('we', 'need', 'to'), 20), (('we’ve', 'got', 'to'), 17), (('medicare', 'for', 'all'), 16), (('we’re', 'going', 'to'), 15)]\n"
     ]
    }
   ],
   "source": [
    "for candidate in freqWordsPair:\n",
    "    print(\"----- \", candidate, \" -----\")\n",
    "#     topWordsVar = candidate.substr(0, text.indexOf(' '))\n",
    "#     topWordsVar = topWordsVar +'TopWords'\n",
    "#     print(topWordsVar)\n",
    "    print(freqWordsPair[candidate][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordProportions = {}\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    wordProportions[candidate] = {}\n",
    "    for word in freqWordsPair[candidate]:\n",
    "        wordProportions[candidate][word[0]] = word[1]/wordCountsPair[candidate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joe Biden           0.212457\n",
       "Elizabeth Warren    0.197232\n",
       "Bernie Sanders      0.191696\n",
       "Pete Buttigieg      0.173010\n",
       "Amy Klobuchar       0.155709\n",
       "Tom Steyer          0.069896\n",
       "Name: speaker, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['speaker'].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpList = []\n",
    "tmpList.append([])\n",
    "tmpList.append([])\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    candidate_data = data.loc[data['speaker'] == candidate]\n",
    "    for line in candidate_data['speech']:\n",
    "        line = line.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\")\n",
    "        wordList = line.split()\n",
    "        line = zip(wordList, wordList[1:], wordList[2:])\n",
    "        for word in line:\n",
    "            tmpList[0].append(word)\n",
    "            tmpList[1].append(candidate)\n",
    "\n",
    "allWords = pd.DataFrame(np.array(tmpList).T,columns=['word','candidate'])\n",
    "# len(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see example code\n",
    "import random\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for i in range(len(allWords)):\n",
    "    tmp.append(({'word':allWords['word'][i]},allWords['candidate'][i]))\n",
    "\n",
    "random.shuffle(tmp)\n",
    "\n",
    "training = tmp[:int(len(allWords) * 0.9)]\n",
    "test = tmp[int(len(allWords) * 0.1):]\n",
    "\n",
    "model_0 = nltk.NaiveBayesClassifier.train(training) ### DIFFERENCE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pete Buttigieg\n",
      "Bernie Sanders\n",
      "Amy Klobuchar\n",
      "Pete Buttigieg\n",
      "Pete Buttigieg\n"
     ]
    }
   ],
   "source": [
    "print(model_0.classify({'word': ('is', 'part', 'of')}))\n",
    "print(model_0.classify({'word':('able', 'to', 'hear')}))\n",
    "print(model_0.classify({'word':('i', 'know', 'we')}))\n",
    "print(model_0.classify({'word':('that', 'we', 'saw')}))\n",
    "print(model_0.classify({'word':('is', 'part', 'of')}))\n",
    "# print(model_0.classify({'word:':''}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824970866955432\n"
     ]
    }
   ],
   "source": [
    "# from example code\n",
    "print(nltk.classify.accuracy(model_0, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "\n",
    "for candidate in top_candidates:\n",
    "    candidate_data = testData.loc[testData['speaker'] == candidate]\n",
    "    for line in candidate_data['speech']:\n",
    "        line = line.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\"-\", \"\").replace(\"\\\"\", \"\")\n",
    "        wordList = line.split()\n",
    "        line = zip(wordList, wordList[1:], wordList[2:])\n",
    "        words = []\n",
    "        for word in line:\n",
    "            words.append(({'word':word}, candidate))\n",
    "        speeches.append(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "#Removing empty or speech with less than 3 words\n",
    "for sp in speeches:\n",
    "    if len(sp)<1:\n",
    "        speeches.remove(sp)\n",
    "for sp in speeches:\n",
    "    if len(sp)<1:\n",
    "        speeches.remove(sp)\n",
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For event of more than one mode\n",
    "\n",
    "from collections import Counter\n",
    "# collision = 0\n",
    "def get_first_mode(a):\n",
    "    c = Counter(a)  \n",
    "    mode_count = max(c.values())\n",
    "    mode = {key for key, count in c.items() if count == mode_count}\n",
    "    first_mode = next(x for x in a if x in mode)\n",
    "    return first_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3108108108108108\n"
     ]
    }
   ],
   "source": [
    "num_def = 0\n",
    "num_correct = 0.0\n",
    "random.shuffle(speeches)\n",
    "for speech in speeches:\n",
    "\n",
    "    word_predictions = []\n",
    "    for word in speech:\n",
    "#         print(word)\n",
    "        word_predictions.append(model_0.classify(word[0]))\n",
    "    \n",
    "    if(len(word_predictions)):\n",
    "        speech_prediction = get_first_mode(word_predictions)\n",
    " \n",
    "    if speech_prediction == speech[0][1]:\n",
    "        num_correct +=1\n",
    "\n",
    "print(num_correct / len(speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
